{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to HTML and XPath\n",
    "\n",
    "_Authors: Kiefer Katovich (SF), Dave Yerrington (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand scraping basics\n",
    "- Get familiar with import.io service\n",
    "- Understand the structure and content of HTML\n",
    "- Utilize XPath to extract information from HTML \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Pre-Work\n",
    "*Before this lesson, you should already:*\n",
    "- Understand basic HTML concepts\n",
    "- Have worked with Beautiful Soup\n",
    "- Have signed up for import.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [HTML](#html)\n",
    "    - [Elements](#elements)\n",
    "    - [Attributes](#attributes)\n",
    "- [What is XPath?](#xpath)\n",
    "    - [Absolute References](#xpath_absolute)\n",
    "    - [Relative References](#xpath_relative)\n",
    "    - [\"Where's Waldo?\" Exercise](#waldo_exercise)\n",
    "- [1 vs. N Selectors](#1_v_n)\n",
    "- [Demo Code](#demo)\n",
    "    - [Scrape DataTau](#scrape_tau)\n",
    "- [Independent Practice](#ind_practice)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "## Introduction: Scraping Overview (10 min)\n",
    "\n",
    "Web scraping is a technique for extracting information from websites. It focuses on transforming unstructured data on the web into structured data that can be stored and analyzed.\n",
    "\n",
    "There are a variety of ways to \"scrape\" what we want from the web:\n",
    "\n",
    "- Using Third-party services (import.io).\n",
    "- By writing our own Python apps that pull HTML documents and parse them.\n",
    "  - Mechanize\n",
    "  - Scrapy\n",
    "  - Requests\n",
    "  - Libxml/XPath\n",
    "  - Beautiful Soup\n",
    "  - Regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion: What Do You Think Would Be the Most Challenging Aspect of Scraping Information?\n",
    "\n",
    "\n",
    "_If you were asked to scrape Craigslist property listings and put them in a DataFrame, what would hold you up?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='html'></a>\n",
    "## HTML Review\n",
    "\n",
    "In the HTML document object model (DOM), everything is a node:\n",
    " * The document itself is a document node.\n",
    " * All HTML elements are element nodes.\n",
    " * All HTML attributes are attribute nodes.\n",
    " * Text inside HTML elements are text nodes.\n",
    " * Comments are comment nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML in IPython\n",
    "\n",
    "You can write HTML-style text in a Jupyter notebook the same way you can style text in markdown. \n",
    "```\n",
    "<p>I am a paragraph.</p>\n",
    "<strong>I am bold.</strong>\n",
    "```\n",
    "<p>I am a paragraph.</p>\n",
    "<strong>I am bold.</strong>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='elements'></a>\n",
    "## Elements\n",
    "Elements begin and end with **opening and closing tags**, which are defined by namespaced, encapsulated strings. \n",
    "\n",
    "```html\n",
    "<title>I am a title.</title>\n",
    "<p>I am a paragraph.</p>\n",
    "<strong>I am bold.</strong>\n",
    "```\n",
    "\n",
    "_Note: The tags **title, p, and strong** are represented here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Element Parent/Child Relationships\n",
    "\n",
    "<img src=\"http://www.htmlgoodies.com/img/2007/06/flowChart2.gif\" width=\"250\">\n",
    "\n",
    "**Elements begin and end in the same namespace, like so:**  `<p></p>`\n",
    "\n",
    "**Elements can have parents and children:**\n",
    "\n",
    "```html\n",
    "<body>\n",
    "    <div>I am inside the parent element.\n",
    "        <div>I am inside a child element.</div>\n",
    "        <div>I am inside another child element.</div>\n",
    "        <div>I am inside yet another child element.</div>\n",
    "    </div>\n",
    "</body>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='attributes'></a>\n",
    "\n",
    "## Element Attributes\n",
    "\n",
    "Elements can also have attributes! Attributes are defined inside **element tags** and can contain data that may be useful to scrape.\n",
    "\n",
    "```html\n",
    "<a href=\"http://lmgtfy.com/?q=html+element+attributes\" title=\"A title\" id=\"web-link\" name=\"hal\">A Simple Link</a>\n",
    "```\n",
    "\n",
    "The **element attributes** of this `<a>` tag element are:\n",
    "- ID\n",
    "- Href\n",
    "- Title\n",
    "- Name\n",
    "\n",
    "This `<a>` tag example will render in your browser like this:\n",
    "> <a href=\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\">A Simple Link</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can You Identify an Attribute, an Element, a Text Item, and a Child Element?\n",
    "\n",
    "```HTML\n",
    "<html>\n",
    "   <title id=\"main-title\">All this scraping is making me itch!</title>\n",
    "   <body>\n",
    "       <h1>Welcome to my Home Page</h1>\n",
    "       <p id=\"welcome-paragraph\" class=\"strong-paragraph\">\n",
    "           <span>Hello friends, let me tell you about this cool hair product.</span>\n",
    "           <ul>\n",
    "              <li>It's cool.</li>\n",
    "              <li>It's fresh.</li>\n",
    "              <li>It can tell the future.</li>\n",
    "              <li>Always be closing.</li>\n",
    "           </ul>\n",
    "       </p>\n",
    "   </body>\n",
    "```\n",
    "\n",
    "**Bonus:** What's missing? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='xpath'></a>\n",
    "\n",
    "## Enter XPath\n",
    "\n",
    "XPath uses path expressions to select nodes or node sets in an HTML/XML document. These path expressions look a lot like the expressions you see when you work with a traditional computer file system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XPath Features\n",
    "\n",
    "XPath includes more than 100 built-in functions to help us select and manipulate HTML or XML documents. XPath has functions for:\n",
    "\n",
    "- String values.\n",
    "- Numeric values.\n",
    "- Date and time comparison.\n",
    "- Sequence manipulation.\n",
    "- Boolean values.\n",
    "- And more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic XPath Expressions\n",
    "\n",
    "XPath comes with a wide array of features, but the basics of selecting data are the most common problems XPath can help you solve.\n",
    "\n",
    "You'll use **XPath** most often for selecting data from HTML documents. There are two ways you can **select elements** within HTML using **XPath**:\n",
    "\n",
    "- Absolute references.\n",
    "- Relative references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='xpath_absolute'></a>\n",
    "## XPath:  Absolute References\n",
    "\n",
    "_For our XPath demonstration, we'll use Scrapy, which is using Libxml under the hood. Libxml provides the basic functionality for XPath expressions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zope in /anaconda2/lib/python2.7/site-packages (4.0b5)\n",
      "Requirement already satisfied: zope.security in /anaconda2/lib/python2.7/site-packages (from zope) (4.2.2)\n",
      "Requirement already satisfied: Acquisition in /anaconda2/lib/python2.7/site-packages (from zope) (4.4.4)\n",
      "Requirement already satisfied: DateTime in /anaconda2/lib/python2.7/site-packages (from zope) (4.2)\n",
      "Requirement already satisfied: waitress in /anaconda2/lib/python2.7/site-packages (from zope) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /anaconda2/lib/python2.7/site-packages (from zope) (38.4.0)\n",
      "Requirement already satisfied: zope.contentprovider in /anaconda2/lib/python2.7/site-packages (from zope) (4.1.0)\n",
      "Requirement already satisfied: zope.globalrequest in /anaconda2/lib/python2.7/site-packages (from zope) (1.4)\n",
      "Requirement already satisfied: zope.lifecycleevent in /anaconda2/lib/python2.7/site-packages (from zope) (4.2.0)\n",
      "Requirement already satisfied: zope.pagetemplate>=4.0.2 in /anaconda2/lib/python2.7/site-packages (from zope) (4.3.0)\n",
      "Requirement already satisfied: zope.size in /anaconda2/lib/python2.7/site-packages (from zope) (4.2.0)\n",
      "Requirement already satisfied: zope.testbrowser in /anaconda2/lib/python2.7/site-packages (from zope) (5.2.4)\n",
      "Requirement already satisfied: zope.component in /anaconda2/lib/python2.7/site-packages (from zope) (4.4.1)\n",
      "Requirement already satisfied: zope.i18nmessageid in /anaconda2/lib/python2.7/site-packages (from zope) (4.1.0)\n",
      "Requirement already satisfied: six in /anaconda2/lib/python2.7/site-packages (from zope) (1.11.0)\n",
      "Requirement already satisfied: zope.contenttype in /anaconda2/lib/python2.7/site-packages (from zope) (4.3.0)\n",
      "Requirement already satisfied: BTrees in /anaconda2/lib/python2.7/site-packages (from zope) (4.5.0)\n",
      "Requirement already satisfied: zope.event in /anaconda2/lib/python2.7/site-packages (from zope) (4.3.0)\n",
      "Requirement already satisfied: zope.tal in /anaconda2/lib/python2.7/site-packages (from zope) (4.3.1)\n",
      "Requirement already satisfied: zope.sequencesort in /anaconda2/lib/python2.7/site-packages (from zope) (4.0.1)\n",
      "Requirement already satisfied: zope.i18n[zcml] in /anaconda2/lib/python2.7/site-packages (from zope) (4.3.1)\n",
      "Requirement already satisfied: RestrictedPython in /anaconda2/lib/python2.7/site-packages (from zope) (3.6.0)\n",
      "Requirement already satisfied: PasteDeploy in /anaconda2/lib/python2.7/site-packages (from zope) (1.5.2)\n",
      "Requirement already satisfied: ZODB in /anaconda2/lib/python2.7/site-packages (from zope) (5.4.0)\n",
      "Requirement already satisfied: zope.publisher in /anaconda2/lib/python2.7/site-packages (from zope) (4.3.2)\n",
      "Requirement already satisfied: zope.browserresource>=3.11 in /anaconda2/lib/python2.7/site-packages (from zope) (4.2.1)\n",
      "Requirement already satisfied: zope.tales>=3.5.0 in /anaconda2/lib/python2.7/site-packages (from zope) (4.2.0)\n",
      "Requirement already satisfied: zope.browser in /anaconda2/lib/python2.7/site-packages (from zope) (2.2.0)\n",
      "Requirement already satisfied: MultiMapping in /anaconda2/lib/python2.7/site-packages (from zope) (4.0)\n",
      "Requirement already satisfied: ipaddress in /anaconda2/lib/python2.7/site-packages (from zope) (1.0.19)\n",
      "Requirement already satisfied: Persistence in /anaconda2/lib/python2.7/site-packages (from zope) (2.13.2)\n",
      "Requirement already satisfied: zope.traversing in /anaconda2/lib/python2.7/site-packages (from zope) (4.2.0)\n",
      "Requirement already satisfied: AccessControl>=4.0b4 in /anaconda2/lib/python2.7/site-packages (from zope) (4.0b4)\n",
      "Requirement already satisfied: ZConfig>=2.9.2 in /anaconda2/lib/python2.7/site-packages (from zope) (3.2.0)\n",
      "Requirement already satisfied: zope.browsermenu in /anaconda2/lib/python2.7/site-packages (from zope) (4.3.0)\n",
      "Requirement already satisfied: zope.location in /anaconda2/lib/python2.7/site-packages (from zope) (4.1.0)\n",
      "Requirement already satisfied: zope.configuration in /anaconda2/lib/python2.7/site-packages (from zope) (4.1.0)\n",
      "Requirement already satisfied: z3c.pt in /anaconda2/lib/python2.7/site-packages (from zope) (3.1.0)\n",
      "Requirement already satisfied: zope.site in /anaconda2/lib/python2.7/site-packages (from zope) (4.1.0)\n",
      "Requirement already satisfied: zope.testing in /anaconda2/lib/python2.7/site-packages (from zope) (4.6.2)\n",
      "Requirement already satisfied: zExceptions>=3.4 in /anaconda2/lib/python2.7/site-packages (from zope) (4.0)\n",
      "Requirement already satisfied: zope.viewlet in /anaconda2/lib/python2.7/site-packages (from zope) (4.1.0)\n",
      "Requirement already satisfied: zope.schema in /anaconda2/lib/python2.7/site-packages (from zope) (4.5.0)\n",
      "Requirement already satisfied: transaction in /anaconda2/lib/python2.7/site-packages (from zope) (2.2.1)\n",
      "Requirement already satisfied: ExtensionClass in /anaconda2/lib/python2.7/site-packages (from zope) (4.3.0)\n",
      "Requirement already satisfied: zope.deferredimport in /anaconda2/lib/python2.7/site-packages (from zope) (4.2.1)\n",
      "Requirement already satisfied: zope.interface>=3.8 in /anaconda2/lib/python2.7/site-packages (from zope) (4.5.0)\n",
      "Requirement already satisfied: zope.processlifetime in /anaconda2/lib/python2.7/site-packages (from zope) (2.2.0)\n",
      "Requirement already satisfied: zope.browserpage>=4.0 in /anaconda2/lib/python2.7/site-packages (from zope) (4.2.0)\n",
      "Requirement already satisfied: zope.proxy in /anaconda2/lib/python2.7/site-packages (from zope) (4.3.0)\n",
      "Requirement already satisfied: DocumentTemplate in /anaconda2/lib/python2.7/site-packages (from zope) (2.13.4)\n",
      "Requirement already satisfied: zope.container in /anaconda2/lib/python2.7/site-packages (from zope) (4.2.1)\n",
      "Requirement already satisfied: zope.exceptions in /anaconda2/lib/python2.7/site-packages (from zope) (4.2.0)\n",
      "Requirement already satisfied: zope.ptresource in /anaconda2/lib/python2.7/site-packages (from zope) (4.1.0)\n",
      "Requirement already satisfied: pytz in /anaconda2/lib/python2.7/site-packages (from DateTime->zope) (2017.3)\n",
      "Requirement already satisfied: zope.cachedescriptors in /anaconda2/lib/python2.7/site-packages (from zope.testbrowser->zope) (4.3.1)\n",
      "Requirement already satisfied: WebTest!=2.0.27,>=2.0.9 in /anaconda2/lib/python2.7/site-packages (from zope.testbrowser->zope) (2.0.30)\n",
      "Requirement already satisfied: WSGIProxy2 in /anaconda2/lib/python2.7/site-packages (from zope.testbrowser->zope) (0.4.4)\n",
      "Requirement already satisfied: persistent>=4.1.0 in /anaconda2/lib/python2.7/site-packages (from BTrees->zope) (4.2.4.2)\n",
      "Requirement already satisfied: python-gettext in /anaconda2/lib/python2.7/site-packages (from zope.i18n[zcml]->zope) (3.0)\n",
      "Requirement already satisfied: zope.deprecation in /anaconda2/lib/python2.7/site-packages (from zope.i18n[zcml]->zope) (4.3.0)\n",
      "Requirement already satisfied: zc.lockfile in /anaconda2/lib/python2.7/site-packages (from ZODB->zope) (1.3.0)\n",
      "Requirement already satisfied: zodbpickle>=0.6.0 in /anaconda2/lib/python2.7/site-packages (from ZODB->zope) (1.0.1)\n",
      "Requirement already satisfied: ZODB3 in /anaconda2/lib/python2.7/site-packages (from Persistence->zope) (3.11.0)\n",
      "Requirement already satisfied: AuthEncoding in /anaconda2/lib/python2.7/site-packages (from AccessControl>=4.0b4->zope) (4.0.0)\n",
      "Requirement already satisfied: Chameleon>=2.4 in /anaconda2/lib/python2.7/site-packages (from z3c.pt->zope) (3.3)\n",
      "Requirement already satisfied: zope.annotation in /anaconda2/lib/python2.7/site-packages (from zope.site->zope) (4.6.0)\n",
      "Requirement already satisfied: zope.structuredtext in /anaconda2/lib/python2.7/site-packages (from DocumentTemplate->zope) (4.2.0)\n",
      "Requirement already satisfied: zope.filerepresentation in /anaconda2/lib/python2.7/site-packages (from zope.container->zope) (4.2.0)\n",
      "Requirement already satisfied: zope.dottedname in /anaconda2/lib/python2.7/site-packages (from zope.container->zope) (4.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /anaconda2/lib/python2.7/site-packages (from WebTest!=2.0.27,>=2.0.9->zope.testbrowser->zope) (4.6.0)\n",
      "Requirement already satisfied: WebOb>=1.2 in /anaconda2/lib/python2.7/site-packages (from WebTest!=2.0.27,>=2.0.9->zope.testbrowser->zope) (1.8.2)\n",
      "Requirement already satisfied: ZEO>=4.0.0dev in /anaconda2/lib/python2.7/site-packages (from ZODB3->Persistence->zope) (5.2.0)\n",
      "Requirement already satisfied: trollius; python_version == \"2.7\" in /anaconda2/lib/python2.7/site-packages (from ZEO>=4.0.0dev->ZODB3->Persistence->zope) (2.2)\n",
      "Requirement already satisfied: zdaemon in /anaconda2/lib/python2.7/site-packages (from ZEO>=4.0.0dev->ZODB3->Persistence->zope) (4.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: futures; python_version == \"2.7\" in /anaconda2/lib/python2.7/site-packages (from ZEO>=4.0.0dev->ZODB3->Persistence->zope) (3.2.0)\n",
      "\u001b[31mgrin 1.2.1 requires argparse>=1.1, which is not installed.\u001b[0m\n",
      "\u001b[31maccesscontrol 4.0b4 has requirement Persistence>=3.0a3, but you'll have persistence 2.13.2 which is incompatible.\u001b[0m\n",
      "\u001b[31maccesscontrol 4.0b4 has requirement RestrictedPython>=4.0a1, but you'll have restrictedpython 3.6.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mproducts-btreefolder2 4.0.0 has requirement Zope2>=4.0a5, but you'll have zope2 2.13.28 which is incompatible.\u001b[0m\n",
      "\u001b[31mproducts-pythonscripts 4.1 has requirement Zope2>=4.0a1, but you'll have zope2 2.13.28 which is incompatible.\u001b[0m\n",
      "\u001b[31mproducts-zcatalog 4.1 has requirement Zope2>=4.0a5, but you'll have zope2 2.13.28 which is incompatible.\u001b[0m\n",
      "\u001b[31mproducts-externalmethod 4.0 has requirement Zope2>=4.0a1, but you'll have zope2 2.13.28 which is incompatible.\u001b[0m\n",
      "\u001b[31mproducts-standardcachemanagers 4.0.1 has requirement Zope2>=4.0a5, but you'll have zope2 2.13.28 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install zope "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34massets\u001b[m\u001b[m\r\n",
      "\u001b[31mbonus-intro-to-html-and-xpath.ipynb\u001b[m\u001b[m\r\n",
      "\u001b[31mintro-to-web-scraping-spiders-with-scrapy.ipynb\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'good']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pip install Scrapy.\n",
    "# Pip install --upgrade zope2.\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "\n",
    "HTML = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <span id=\"only-span\">good</span>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "# The same thing, but an \"absolute\" reference.\n",
    "Selector(text=HTML).xpath('/html/body/span/text()').extract()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='xpath_relative'></a>\n",
    "## XPath: Relative References\n",
    "\n",
    "Relative references in XPath match the \"ends\" of structures. Because there's only a single `span` element, `//span/text()` matches **one element**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'good']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Selector(text=HTML).xpath('//span/text()').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Attributes\n",
    "\n",
    "Attributes can be found **within a tag**, such as `id=\"only-span\"` within our `span` attribute. We can get the attribute by using the `@` symbol **after** the **element reference**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'only-span']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Selector(text=HTML).xpath('//span/@id').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='waldo_exercise'></a>\n",
    "## Where's Waldo? — XPath Edition (~10 min)\n",
    "\n",
    "In this example, we will find Waldo together. Find Waldo as:\n",
    "\n",
    "- An element.\n",
    "- An attribute.\n",
    "- A text element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        \n",
    "        <ul id=\"waldo\">\n",
    "            <li class=\"waldo\">\n",
    "                <span> yo I'm not here</span>\n",
    "            </li>\n",
    "            <li class=\"waldo\">Height:  ???</li>\n",
    "            <li class=\"waldo\">Weight:  ???</li>\n",
    "            <li class=\"waldo\">Last Location:  ???</li>\n",
    "            <li class=\"nerds\">\n",
    "                <div class=\"alpha\">Bill Gates</div>\n",
    "                <div class=\"alpha\">Zuckerberg</div>\n",
    "                <div class=\"beta\">Theil</div>\n",
    "                <div class=\"animal\">Parker</div>\n",
    "            </li>\n",
    "        </ul>\n",
    "        \n",
    "        <ul id=\"tim\">\n",
    "            <li class=\"tdawg\">\n",
    "                <span>yo im here</span>\n",
    "            </li>\n",
    "        </ul>\n",
    "        <li>stuff</li>\n",
    "        <li>stuff2</li>\n",
    "        \n",
    "        <div id=\"cooldiv\">\n",
    "            <span class=\"dsi-rocks\">\n",
    "               YO!\n",
    "            </span>\n",
    "        </div>\n",
    "        \n",
    "        \n",
    "        <waldo>Waldo</waldo>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'<waldo>Waldo</waldo>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Text contents of the element Waldo:\n",
    "#print Selector(text=HTML).xpath('/html/body/waldo/text()').extract()\n",
    "\n",
    "# # Contents of all class attributes named Waldo:\n",
    "#print Selector(text=HTML).xpath('//*[@class=\"waldo\"]').extract()\n",
    "\n",
    "# # Contents of all attributes named Waldo:\n",
    "# print Selector(text=HTML).xpath('//*[@*=\"waldo\"]').extract()\n",
    "\n",
    "# # Gets everything around the text element Waldo:\n",
    "Selector(text=HTML).xpath(\"//*[text()='Waldo']\").extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1_v_n'></a>\n",
    "\n",
    "## 1 vs. N Selections\n",
    "\n",
    "When selecting elements via relative reference, it's possible that you'll select multiple items. But, it's still possible to select single items if you're specific enough.\n",
    "\n",
    "**Singular Reference**\n",
    "- **Index** starts at **1**.\n",
    "- Selections by offset.\n",
    "- Selections by \"first\" or \"last.\"\n",
    "- Selections by **unique attribute value**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'5,233.42']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "    \n",
    "        <!-- Search Results -->\n",
    "        <div class=\"search-result\">\n",
    "           <a href=\"https://www.youtube.com/watch?v=751hUX_q0Do\" title=\"Rappin with Gas\">Rapping with gas</a>\n",
    "           <span class=\"link-details\">This is a great video about gas.</span>\n",
    "        </div>\n",
    "        <div class=\"search-result\">\n",
    "           <a href=\"https://www.youtube.com/watch?v=97byWqi-zsI\" title=\"Casio Rapmap\">The Rapmaster</a>\n",
    "           <span class=\"link-details\">My first synth ever.</span>\n",
    "        </div>\n",
    "        <div class=\"search-result\">\n",
    "           <a href=\"https://www.youtube.com/watch?v=TSwqnR327fk\" title=\"Cinco Products\">Cinco Midi Organizer</a>\n",
    "           <span class=\"link-details\">Midi files at the speed of light.</span>\n",
    "        </div>\n",
    "        <div class=\"search-result\">\n",
    "           <a href=\"https://www.youtube.com/watch?v=8TCxE0bWQeQ\" title=\"Baddest Gates\">BBG Baddest Moments</a>\n",
    "           <span class=\"link-details\">It's tough to be a gangster.</span>\n",
    "        </div>\n",
    "        \n",
    "        <!-- Page stats -->\n",
    "        <div class=\"page-stats-container\">\n",
    "            <li class=\"item\" id=\"pageviews\">1,333,443</li>\n",
    "            <li class=\"item\" id=\"somethingelse\">bla</li>\n",
    "            <li class=\"item\" id=\"last-viewed\">01-22-2016</li>\n",
    "            <li class=\"item\" id=\"views-per-hour\">1,532</li>\n",
    "            <li class=\"item\" id=\"kiefer-views-per-hour\">5,233.42</li>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "span = Selector(text=HTML).xpath('/html/body/div/li[@id=\"kiefer-views-per-hour\"]/text()').extract()\n",
    "span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting the first element in a series of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'<span class=\"link-details\">Midi files at the speed of light.</span>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spans = Selector(text=HTML).xpath('//span').extract()\n",
    "spans[2]\n",
    "# '//span' refers to all the span elements. \n",
    "# spans[0] = <span class=\"link-details\">This is a great video about gas.</span>\n",
    "# spans[1] = <span class=\"link-details\">My first synth ever.</span>\n",
    "# spans[2] = <span class=\"link-details\">Midi files at the speed of light.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting the last element in a series of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'<span class=\"link-details\">It\\'s tough to be a gangster.</span>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spans = Selector(text=HTML).xpath('//span').extract()\n",
    "spans[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unicode"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spans[-1])\n",
    "# It's not a list. It's an object called unicode. The above label says 'a series of elements.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting all elements matching a selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'<span class=\"link-details\">This is a great video about gas.</span>',\n",
       " u'<span class=\"link-details\">My first synth ever.</span>',\n",
       " u'<span class=\"link-details\">Midi files at the speed of light.</span>',\n",
       " u'<span class=\"link-details\">It\\'s tough to be a gangster.</span>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Selector(text=HTML).xpath('//span').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Elements Matching an _Attribute_\n",
    "\n",
    "This will be one of the most common ways you will select items. HTML DOM elements will be differentiated based on their \"class\" and \"ID\" variables. Mainly, these types of attributes are used by web developers to refer to specific elements or a broad set of elements to apply visual characteristics to using CSS.\n",
    "\n",
    "```HTML \n",
    "//element[@attribute=\"value\"]\n",
    "```\n",
    "\n",
    "**Generally:**\n",
    "\n",
    "- \"Class\" attributes within elements usually refer to multiple items.\n",
    "- \"ID\" attributes are supposed to be unique but aren't always.\n",
    "\n",
    "_CSS stands for cascading style sheets. These are used to abstract the definition of visual elements on a micro and macro scale for the web. They are also our best friend as data miners. They give us strong hints and cues as to how a web document is structured._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='demo'></a>\n",
    "\n",
    "## Let's Code\n",
    "\n",
    " - How can we get a series of only text items for the page statistics section of our page?\n",
    " - We want to know only how many times Kiefer views the YouTube videos page per hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><em>HAVE TO CHECK THE ANSWER FOR THIS. THIS ANSWER DOES NOT SEEM TO BE IT.</em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Selector xpath=None data=u'<html>\\n    <body>\\n    \\n        <!-- Sear'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all of the text elements for the page statistics section.\n",
    "Selector(text=HTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Selector(text=HTML).xpath('/html/body/span/text()').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'5,233.42']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Option 1: Get only the text for \"Kiefer's\" number of views per hour.\n",
    "Selector(text=HTML).xpath('//div[@class=\"page-stats-container\"]/li[5]/text()').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'5,233.42']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Option 2: Get only the text for \"Kiefer's\" number of views per hour.\n",
    "Selector(text=HTML).xpath('//li[@id=\"kiefer-views-per-hour\"]/text()').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Quick Note:  Requests\n",
    "\n",
    "The requests module is the gateway to interacting with the web using Python. We can:\n",
    "\n",
    " - Fetch web documents as strings.\n",
    " - Decode JSON.\n",
    " - Perform basic data munging with web documents.\n",
    " - Download static files that are not text:\n",
    "  - Images.\n",
    "  - Videos.\n",
    "  - Binary data.\n",
    "\n",
    "Take some time and read up on requests:\n",
    "\n",
    "http://docs.python-requests.org/en/master/user/quickstart/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scrape_tau'></a>\n",
    "\n",
    "## Let's Scrape DataTau Headlines\n",
    "\n",
    "DataTau is a great site for data science news. Let's take their headlines using Python **requests** and practice selecting various elements.\n",
    "\n",
    "Using the <a href=\"https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl?hl=en\">XPath Helper Chrome plugin</a> _(cmd-shift-x)_ and the Chrome Inspect feature, let's explore the structure of the page.\n",
    "\n",
    "_Here's a <a href=\"https://www.youtube.com/watch?v=i2Li1vnv09U\">concise video</a> that demonstrates the basic Inspect feature in Chrome._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'<html><head><link rel=\"stylesheet\" type=\"text/css\" href=\"news.css\">\\n<link rel=\"shortcut icon\" href=\"http://www.iconj.com/ico/d/x/dxo02ap56v.ico\">\\n<scr'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please only run this frame once to avoid hitting the site too hard all at once.\n",
    "import requests\n",
    "\n",
    "response = requests.get(\"http://www.datatau.com\")\n",
    "HTML = response.text  \n",
    "HTML[0:150]           # View the first 150 characters of the HTML index document for DataTau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Only the Headlines\n",
    "\n",
    "We'll use XPath Helper to inspect the markup that comprises the **title** to find a pattern. As there is more than one **title**, we expect to find a series of elements representing the **title** data we're interested in.\n",
    "\n",
    "![](https://snag.gy/m4K3UE.jpg)\n",
    "\n",
    "In this example, we are referencing the **first center**, **third table row (`tr[3]`)** within the second **`td` having a class of `\"title\"` (`td[@class=\"title\"][2]`)**, and the anchor tag within a **(`a/text()`)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'How to use Machine Learning and Quilt to Identify Buildings in Satellite Images',\n",
       " u'1.1 Billion Taxi Rides with SQLite, Parquet & HDFS',\n",
       " u'Correlating stock returns using Python, visualizing in Seaborn',\n",
       " u'Spatiotemporal modeling with R',\n",
       " u'Scaling Pandas to the Billions with Ibis and MapD',\n",
       " u'Introducing plotly.py 3.0.0',\n",
       " u'Tutorial: Setting up an IPFS peer, part IV',\n",
       " u'Introducing the IRONdb Prometheus Adapter',\n",
       " u'Python libraries and packages for Data Scientists (the 5 most important ones)',\n",
       " u'Announcing MapD 4.0: Geospatial, Role-Based Permissions and Updates/Deletes']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "titles = Selector(text=HTML).xpath('//td[@class=\"title\"]/a/text()').extract()\n",
    "titles[0:10] # The first five titles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Do We Get the URLs From the Titles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'/x?fnid=KpQaUb0iTO',\n",
       " u'https://blog.alookanalytics.com/2018/06/11/geolocated-nearest-neighbors-in-product-campaign-targeting/',\n",
       " u'https://blog.insightdatascience.com/reinforcement-learning-from-scratch-819b65f074d8',\n",
       " u'https://www.medium.com/activewizards-machine-learning-company/top-20-r-libraries-for-data-science-in-2018-infographic-956f8419f883/',\n",
       " u'https://www.medium.com/activewizards-machine-learning-company/top-7-data-science-use-cases-in-finance-303c05a3cb58/',\n",
       " u'https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05',\n",
       " u'https://www.medium.com/data-science-school/practical-apache-spark-in-10-minutes-part-2-rdd-8e34c7663d6d/',\n",
       " u'http://www.sharpsightlabs.com/blog/key-for-mastering-data-science/',\n",
       " u'https://medium.com/nanonets/how-we-flew-a-drone-to-monitor-construction-projects-in-africa-using-deep-learning-b792f5c9c471',\n",
       " u'http://datasciencedigest.flyelephant.net/issues/datascience-digest-issue-13-88803',\n",
       " u'https://medium.com/textileio/decentralized-code-distribution-for-the-future-of-open-source-2dc58f1153b2',\n",
       " u'https://github.com/nateraw/Lda2vec-Tensorflow',\n",
       " u'http://outlace.com/TensorNets1.html',\n",
       " u'https://courses.data36.com/p/the-junior-data-scientist-s-first-month-online-video-course',\n",
       " u'https://www.interviewqs.com/',\n",
       " u'https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219',\n",
       " u'https://medium.com/acing-ai/walmart-data-science-interview-questions-acing-the-ai-interview-a775b264b015',\n",
       " u'https://medium.com/acing-ai/artificial-intelligence-is-the-bicycle-for-our-technology-my-udacity-ama-a7068afc6ed9',\n",
       " u'http://www.theanalyticslab.nl/2018/06/24/telegram-messages/',\n",
       " u'https://statsbot.co/blog/data-team',\n",
       " u'https://thenewstack.io/the-good-bad-and-ugly-apache-spark-for-data-science-work/',\n",
       " u'https://www.mapd.com/blog/announcing-mapd-4.0/',\n",
       " u'https://data36.com/python-libraries-packages-data-scientists/',\n",
       " u'https://www.circonus.com/2018/06/prometheus-adapter/',\n",
       " u'https://medium.com/textileio/tutorial-setting-up-an-ipfs-peer-part-iv-1595d4ba221b',\n",
       " u'https://medium.com/@plotlygraphs/introducing-plotly-3-0-0-7bb1333f69c6',\n",
       " u'https://www.mapd.com/blog/scaling-pandas-to-the-billions-with-ibis-and-mapd',\n",
       " u'https://github.com/Italosayan/P-P-P',\n",
       " u'https://www.interviewqs.com/blog/py_stock_correlation',\n",
       " u'http://tech.marksblogg.com/billion-nyc-taxi-rides-sqlite-parquet-hdfs.html',\n",
       " u'https://blog.insightdatascience.com/how-to-use-machine-learning-and-quilt-to-identify-buildings-in-satellite-images-aee4e08ab0f3']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = Selector(text=HTML).xpath('//td[@class=\"title\"]/a/@href').extract()\n",
    "urls[::-1]\n",
    "#<a href=\"http://tech.marksblogg.com/faster-queries-google-cloud-dataproc.html\"> Thirty-three-times-faster queries on Google Cloud's Dataproc using Facebook's Presto.</a>\n",
    "# titles[0:5] # The first five titles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can we get the site domain after the title within the parentheses (i.e., stitchfix.com)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = Selector(text=HTML).xpath(\"//span[@class='comhead']/text()\").extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u' (insightdatascience.com) ',\n",
       " u' (marksblogg.com) ',\n",
       " u' (interviewqs.com) ',\n",
       " u' (github.com) ',\n",
       " u' (mapd.com) ']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domains[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How about the points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><em>NOT SURE ABOUT THIS. CHECK IF TIME ALLOWS</em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'3 points', u'3 points', u'2 points', u'4 points', u'3 points']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = Selector(text=HTML).xpath('//td[@class=\"subtext\"]/span/text()').extract()\n",
    "points[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How about the \"more link?\"\n",
    "Hint: You can use `element[text()='exact text']` to find text element matching specific text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><em>NOT SURE ABOUT THIS. CHECK IF TIME ALLOWS</em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'/x?fnid=KpQaUb0iTO']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_link = Selector(text=HTML).xpath('//a[text()=\"More\"]/@href').extract()\n",
    "next_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ind_practice'></a>\n",
    "\n",
    "## Independent Practice/Lab\n",
    "\n",
    "For the next 30 minutes, try to grab the following:\n",
    "\n",
    "- Story titles.\n",
    "- Story URL (href).\n",
    "- Domain.\n",
    "- Points.\n",
    "\n",
    "Stretch:\n",
    "- Author.\n",
    "- Comment count.\n",
    "\n",
    "Then, put your results into a DataFrame.\n",
    "\n",
    "- Perform a basic analysis of domains and point distributions.\n",
    "\n",
    "**Bonus**\n",
    "\n",
    "Automatically find the next \"more\" link and mine the next page(s) until none exist  Logically, you can code each page with this pseudocode:\n",
    "\n",
    "1) Does the next link exist (a tag with `text == \"More\"`)?\n",
    "2) Fetch URL, prepended with domain (`datatau.com/(extracted link here)`).\n",
    "3) Parse the page with `Selector(text=HTML).xpath('').extract()` to find the elements.\n",
    "4) Add to DataFrame.\n",
    "\n",
    "_Note: You might want to set a limit — something like 2–3 total requests per attempt — to avoid unnecessary transfer._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching http://www.datatau.com/x?fnid=fbvGMQO7zZ...\n",
      "Fetching http://www.datatau.com/x?fnid=0bj2gAACbX...\n",
      "Fetching http://www.datatau.com/x?fnid=PfksdqnRDz...\n",
      "Fetching http://www.datatau.com/x?fnid=o6vRSWANte...\n",
      "Fetching http://www.datatau.com/x?fnid=eL5r2C8sVC...\n",
      "Fetching http://www.datatau.com/x?fnid=MCFmteEne2...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>authors</th>\n",
       "      <th>comments</th>\n",
       "      <th>domains</th>\n",
       "      <th>links</th>\n",
       "      <th>points</th>\n",
       "      <th>titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>hiimtomi</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(medium.com)</td>\n",
       "      <td>https://medium.com/@datalab/aspiring-data-scie...</td>\n",
       "      <td>5 points</td>\n",
       "      <td>Aspiring Data Scientists! Here are some great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>hiimtomi</td>\n",
       "      <td>1 comment</td>\n",
       "      <td>(data36.com)</td>\n",
       "      <td>https://data36.com/sql-for-data-analysis-works...</td>\n",
       "      <td>4 points</td>\n",
       "      <td>SQL For Data Analysis Workshop (for Beginners)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>jane_brown</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(dwhsys.com)</td>\n",
       "      <td>https://dwhsys.com/2017/03/25/apache-zeppelin-...</td>\n",
       "      <td>23 points</td>\n",
       "      <td>Apache Zeppelin vs. Jupyter Notebook: comparis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ankit</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(udemy.com)</td>\n",
       "      <td>https://www.udemy.com/manipulate-excel-file-fr...</td>\n",
       "      <td>26 points</td>\n",
       "      <td>Automate Excel, Word, PDF, HTML Web Scraping f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>kghamilton</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(github.com)</td>\n",
       "      <td>https://github.com/axibase/atsd-use-cases/blob...</td>\n",
       "      <td>2 points</td>\n",
       "      <td>Aging America: Modeling Birth Trends in the Un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>maddad</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(knowtechie.com)</td>\n",
       "      <td>https://knowtechie.com/mobile-app-business-howto/</td>\n",
       "      <td>2 points</td>\n",
       "      <td>A 3-step guide to starting your mobile app bus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>wilsstar007</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(packtpub.com)</td>\n",
       "      <td>https://datahub.packtpub.com/deep-learning/15-...</td>\n",
       "      <td>7 points</td>\n",
       "      <td>15 Useful Python Libraries to make your Data S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>tmostak</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(mapd.com)</td>\n",
       "      <td>https://www.mapd.com/blog/exploring-google-ana...</td>\n",
       "      <td>5 points</td>\n",
       "      <td>Exploring Google Analytics data with MapD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>drachenbach</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(github.io)</td>\n",
       "      <td>https://drachenbach.github.io/blog/2018/03/12/...</td>\n",
       "      <td>6 points</td>\n",
       "      <td>The Intuition behind Embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>eoberg</td>\n",
       "      <td>1 comment</td>\n",
       "      <td>(medium.com)</td>\n",
       "      <td>https://medium.com/indeed-data-science/theres-...</td>\n",
       "      <td>4 points</td>\n",
       "      <td>Data Science job searching behavior - There's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>iheartai</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(aiworkbox.com)</td>\n",
       "      <td>https://www.aiworkbox.com/lessons/check-for-el...</td>\n",
       "      <td>2 points</td>\n",
       "      <td>Check For Element Wise Equality Between Two Py...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>summarizebotdev</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(summarizebot.com)</td>\n",
       "      <td>http://www.summarizebot.com/summarization_busi...</td>\n",
       "      <td>6 points</td>\n",
       "      <td>Natural language processing and web scraping API</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>bon</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(arxiv.org)</td>\n",
       "      <td>https://arxiv.org/abs/1803.10768</td>\n",
       "      <td>2 points</td>\n",
       "      <td>The Unreasonable Effectivness of Deep Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>bull</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(drivendata.org)</td>\n",
       "      <td>https://www.drivendata.org/competitions/</td>\n",
       "      <td>8 points</td>\n",
       "      <td>3 new comps for forecasting, anomalies and opt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>kantord</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(github.io)</td>\n",
       "      <td>https://kantord.github.io/just-dashboard/</td>\n",
       "      <td>5 points</td>\n",
       "      <td>Just-dashboard: Create shareable interactive d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>e_ameisen</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(insightdatascience.com)</td>\n",
       "      <td>https://blog.insightdatascience.com/hero2vec-d...</td>\n",
       "      <td>8 points</td>\n",
       "      <td>Predicting e-sports winners with Machine Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>hiimtomi</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(data36.com)</td>\n",
       "      <td>https://data36.com/create-table-sql/</td>\n",
       "      <td>2 points</td>\n",
       "      <td>How to Create a Table in SQL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>hiimtomi</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(data36.com)</td>\n",
       "      <td>https://data36.com/data-science-career-questio...</td>\n",
       "      <td>6 points</td>\n",
       "      <td>Is Data Science For You?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>hiimtomi</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(hackernoon.com)</td>\n",
       "      <td>https://hackernoon.com/aspiring-data-scientist...</td>\n",
       "      <td>14 points</td>\n",
       "      <td>Aspiring Data Scientists! Start to learn Stati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>deeplearnting</td>\n",
       "      <td>1 comment</td>\n",
       "      <td>(deeplearningtrack.com)</td>\n",
       "      <td>https://www.deeplearningtrack.com/single-post/...</td>\n",
       "      <td>16 points</td>\n",
       "      <td>Setting up spark on google cloud made easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>bike_visual</td>\n",
       "      <td>3 comments</td>\n",
       "      <td>(visualization.bike)</td>\n",
       "      <td>https://www.visualization.bike/</td>\n",
       "      <td>48 points</td>\n",
       "      <td>The most complete bike sharing visualization a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>albanotte</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(statsbot.co)</td>\n",
       "      <td>https://statsbot.co/blog/select-random-rows-sql/</td>\n",
       "      <td>2 points</td>\n",
       "      <td>Scalable Select of Random Rows in SQL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>janvdp</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(zerotosingularity.com)</td>\n",
       "      <td>https://www.zerotosingularity.com/posts/course...</td>\n",
       "      <td>7 points</td>\n",
       "      <td>Running your Coursera (and all other) Jupyter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>dacod3r</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(github.com)</td>\n",
       "      <td>https://github.com/tmadl/sklearn-interpretable...</td>\n",
       "      <td>7 points</td>\n",
       "      <td>Simplified tree-based classifier &amp; regressor f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>andrewxhill</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(sxsw.com)</td>\n",
       "      <td>https://schedule.sxsw.com/2018/events/PP73084</td>\n",
       "      <td>3 points</td>\n",
       "      <td>SXSW panel on offline data science, decentrali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>aldamiz</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(hackernoon.com)</td>\n",
       "      <td>https://hackernoon.com/how-we-grew-from-0-to-4...</td>\n",
       "      <td>3 points</td>\n",
       "      <td>Growing from 0 to 4M users on our fashion app ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>jameslee</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(statsbot.co)</td>\n",
       "      <td>https://statsbot.co/ltv-ebook</td>\n",
       "      <td>9 points</td>\n",
       "      <td>Estimating customer lifetime value with SQL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>randyzwitch</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(randyzwitch.com)</td>\n",
       "      <td>http://randyzwitch.com/mapd-pjm-electricity-data/</td>\n",
       "      <td>7 points</td>\n",
       "      <td>Getting Started With MapD, Part 2: Electricity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>zozozzzoya</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(datalore.io)</td>\n",
       "      <td>https://blog.datalore.io/february-data-digest/</td>\n",
       "      <td>4 points</td>\n",
       "      <td>Datalore's February Data Digest - on deep lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>mmq</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(medium.com)</td>\n",
       "      <td>https://medium.com/polyaxon/jupyter-notebooks-...</td>\n",
       "      <td>4 points</td>\n",
       "      <td>Jupyter notebooks and tensorboard on Polyaxon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0</td>\n",
       "      <td>ddrum001</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(insightdatascience.com)</td>\n",
       "      <td>https://blog.insightdatascience.com/how-to-use...</td>\n",
       "      <td>3 points</td>\n",
       "      <td>How to use Machine Learning and Quilt to Ident...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1</td>\n",
       "      <td>marklit</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(marksblogg.com)</td>\n",
       "      <td>http://tech.marksblogg.com/billion-nyc-taxi-ri...</td>\n",
       "      <td>3 points</td>\n",
       "      <td>1.1 Billion Taxi Rides with SQLite, Parquet &amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>2</td>\n",
       "      <td>ddi_1</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(interviewqs.com)</td>\n",
       "      <td>https://www.interviewqs.com/blog/py_stock_corr...</td>\n",
       "      <td>2 points</td>\n",
       "      <td>Correlating stock returns using Python, visual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>3</td>\n",
       "      <td>italosay</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(github.com)</td>\n",
       "      <td>https://github.com/Italosayan/P-P-P</td>\n",
       "      <td>4 points</td>\n",
       "      <td>Spatiotemporal modeling with R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>4</td>\n",
       "      <td>randyzwitch</td>\n",
       "      <td>2 comments</td>\n",
       "      <td>(mapd.com)</td>\n",
       "      <td>https://www.mapd.com/blog/scaling-pandas-to-th...</td>\n",
       "      <td>3 points</td>\n",
       "      <td>Scaling Pandas to the Billions with Ibis and MapD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>5</td>\n",
       "      <td>jeremypmason</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(medium.com)</td>\n",
       "      <td>https://medium.com/@plotlygraphs/introducing-p...</td>\n",
       "      <td>3 points</td>\n",
       "      <td>Introducing plotly.py 3.0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>6</td>\n",
       "      <td>andrewxhill</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(medium.com)</td>\n",
       "      <td>https://medium.com/textileio/tutorial-setting-...</td>\n",
       "      <td>3 points</td>\n",
       "      <td>Tutorial: Setting up an IPFS peer, part IV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>7</td>\n",
       "      <td>Crusso3</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(circonus.com)</td>\n",
       "      <td>https://www.circonus.com/2018/06/prometheus-ad...</td>\n",
       "      <td>2 points</td>\n",
       "      <td>Introducing the IRONdb Prometheus Adapter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>8</td>\n",
       "      <td>jukatan</td>\n",
       "      <td>1 comment</td>\n",
       "      <td>(data36.com)</td>\n",
       "      <td>https://data36.com/python-libraries-packages-d...</td>\n",
       "      <td>5 points</td>\n",
       "      <td>Python libraries and packages for Data Scienti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>9</td>\n",
       "      <td>randyzwitch</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(mapd.com)</td>\n",
       "      <td>https://www.mapd.com/blog/announcing-mapd-4.0/</td>\n",
       "      <td>4 points</td>\n",
       "      <td>Announcing MapD 4.0: Geospatial, Role-Based Pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>10</td>\n",
       "      <td>scotthajek</td>\n",
       "      <td>1 comment</td>\n",
       "      <td>(thenewstack.io)</td>\n",
       "      <td>https://thenewstack.io/the-good-bad-and-ugly-a...</td>\n",
       "      <td>3 points</td>\n",
       "      <td>The Good, Bad and Ugly: Apache Spark for Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>11</td>\n",
       "      <td>analytics</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(statsbot.co)</td>\n",
       "      <td>https://statsbot.co/blog/data-team</td>\n",
       "      <td>6 points</td>\n",
       "      <td>How to balance the load on a data team</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>12</td>\n",
       "      <td>kromme</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(theanalyticslab.nl)</td>\n",
       "      <td>http://www.theanalyticslab.nl/2018/06/24/teleg...</td>\n",
       "      <td>3 points</td>\n",
       "      <td>Let R/Python send messages when the algorithms...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>13</td>\n",
       "      <td>vimarshk</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(medium.com)</td>\n",
       "      <td>https://medium.com/acing-ai/artificial-intelli...</td>\n",
       "      <td>2 points</td>\n",
       "      <td>Artificial Intelligence is the Bicycle for our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>14</td>\n",
       "      <td>vimarshk</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(medium.com)</td>\n",
       "      <td>https://medium.com/acing-ai/walmart-data-scien...</td>\n",
       "      <td>6 points</td>\n",
       "      <td>Walmart Data Science Interview Questions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>15</td>\n",
       "      <td>kmax12</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(towardsdatascience.com)</td>\n",
       "      <td>https://towardsdatascience.com/automated-featu...</td>\n",
       "      <td>5 points</td>\n",
       "      <td>Automated Feature Engineering in Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>16</td>\n",
       "      <td>ddi_1</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(interviewqs.com)</td>\n",
       "      <td>https://www.interviewqs.com/</td>\n",
       "      <td>9 points</td>\n",
       "      <td>Practice DS interview questions through email ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>17</td>\n",
       "      <td>jukatan</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(data36.com)</td>\n",
       "      <td>https://courses.data36.com/p/the-junior-data-s...</td>\n",
       "      <td>2 points</td>\n",
       "      <td>The Junior Data Scientist's First Month (Class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>18</td>\n",
       "      <td>outlace</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(outlace.com)</td>\n",
       "      <td>http://outlace.com/TensorNets1.html</td>\n",
       "      <td>4 points</td>\n",
       "      <td>Tensor Networks and the Nature of Non-Linearity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>19</td>\n",
       "      <td>italosay</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(github.com)</td>\n",
       "      <td>https://github.com/nateraw/Lda2vec-Tensorflow</td>\n",
       "      <td>5 points</td>\n",
       "      <td>Simple topic modeling using tensorflow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>20</td>\n",
       "      <td>andrewxhill</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(medium.com)</td>\n",
       "      <td>https://medium.com/textileio/decentralized-cod...</td>\n",
       "      <td>3 points</td>\n",
       "      <td>Decentralized code distribution for the future...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>21</td>\n",
       "      <td>flyelephant</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(flyelephant.net)</td>\n",
       "      <td>http://datasciencedigest.flyelephant.net/issue...</td>\n",
       "      <td>9 points</td>\n",
       "      <td>DataScience Digest - Issue #13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>22</td>\n",
       "      <td>nanonets</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(medium.com)</td>\n",
       "      <td>https://medium.com/nanonets/how-we-flew-a-dron...</td>\n",
       "      <td>12 points</td>\n",
       "      <td>How to easily automate Drone-based monitoring ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>23</td>\n",
       "      <td>SharpSightLabs</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(sharpsightlabs.com)</td>\n",
       "      <td>http://www.sharpsightlabs.com/blog/key-for-mas...</td>\n",
       "      <td>3 points</td>\n",
       "      <td>A key for mastering data science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>24</td>\n",
       "      <td>tbugaevskii</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(medium.com)</td>\n",
       "      <td>https://www.medium.com/data-science-school/pra...</td>\n",
       "      <td>26 points</td>\n",
       "      <td>Practical Apache Spark in 10 minutes. Part 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>25</td>\n",
       "      <td>asdasdas</td>\n",
       "      <td>2 comments</td>\n",
       "      <td>(medium.com)</td>\n",
       "      <td>https://medium.com/nanonets/topic-modeling-wit...</td>\n",
       "      <td>19 points</td>\n",
       "      <td>How to easily do Topic Modeling with LSA, PSLA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>26</td>\n",
       "      <td>tbugaevskii</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(medium.com)</td>\n",
       "      <td>https://www.medium.com/activewizards-machine-l...</td>\n",
       "      <td>30 points</td>\n",
       "      <td>Top 7 Data Science Use Cases in Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>27</td>\n",
       "      <td>tbugaevskii</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(medium.com)</td>\n",
       "      <td>https://www.medium.com/activewizards-machine-l...</td>\n",
       "      <td>21 points</td>\n",
       "      <td>Top 20 R Libraries for Data Science in 2018 [I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>28</td>\n",
       "      <td>e_ameisen</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(insightdatascience.com)</td>\n",
       "      <td>https://blog.insightdatascience.com/reinforcem...</td>\n",
       "      <td>8 points</td>\n",
       "      <td>Reinforcement Learning from scratch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>29</td>\n",
       "      <td>adam_alook</td>\n",
       "      <td>discuss</td>\n",
       "      <td>(alookanalytics.com)</td>\n",
       "      <td>https://blog.alookanalytics.com/2018/06/11/geo...</td>\n",
       "      <td>5 points</td>\n",
       "      <td>Geolocated nearest neighbors in product campai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index          authors    comments                     domains  \\\n",
       "0        0         hiimtomi     discuss               (medium.com)    \n",
       "1        1         hiimtomi   1 comment               (data36.com)    \n",
       "2        2       jane_brown     discuss               (dwhsys.com)    \n",
       "3        3            ankit     discuss                (udemy.com)    \n",
       "4        4       kghamilton     discuss               (github.com)    \n",
       "5        5           maddad     discuss           (knowtechie.com)    \n",
       "6        6      wilsstar007     discuss             (packtpub.com)    \n",
       "7        7          tmostak     discuss                 (mapd.com)    \n",
       "8        8      drachenbach     discuss                (github.io)    \n",
       "9        9           eoberg   1 comment               (medium.com)    \n",
       "10      10         iheartai     discuss            (aiworkbox.com)    \n",
       "11      11  summarizebotdev     discuss         (summarizebot.com)    \n",
       "12      12              bon     discuss                (arxiv.org)    \n",
       "13      13             bull     discuss           (drivendata.org)    \n",
       "14      14          kantord     discuss                (github.io)    \n",
       "15      15        e_ameisen     discuss   (insightdatascience.com)    \n",
       "16      16         hiimtomi     discuss               (data36.com)    \n",
       "17      17         hiimtomi     discuss               (data36.com)    \n",
       "18      18         hiimtomi     discuss           (hackernoon.com)    \n",
       "19      19    deeplearnting   1 comment    (deeplearningtrack.com)    \n",
       "20      20      bike_visual  3 comments       (visualization.bike)    \n",
       "21      21        albanotte     discuss              (statsbot.co)    \n",
       "22      22           janvdp     discuss    (zerotosingularity.com)    \n",
       "23      23          dacod3r     discuss               (github.com)    \n",
       "24      24      andrewxhill     discuss                 (sxsw.com)    \n",
       "25      25          aldamiz     discuss           (hackernoon.com)    \n",
       "26      26         jameslee     discuss              (statsbot.co)    \n",
       "27      27      randyzwitch     discuss          (randyzwitch.com)    \n",
       "28      28       zozozzzoya     discuss              (datalore.io)    \n",
       "29      29              mmq     discuss               (medium.com)    \n",
       "..     ...              ...         ...                         ...   \n",
       "180      0         ddrum001     discuss   (insightdatascience.com)    \n",
       "181      1          marklit     discuss           (marksblogg.com)    \n",
       "182      2            ddi_1     discuss          (interviewqs.com)    \n",
       "183      3         italosay     discuss               (github.com)    \n",
       "184      4      randyzwitch  2 comments                 (mapd.com)    \n",
       "185      5     jeremypmason     discuss               (medium.com)    \n",
       "186      6      andrewxhill     discuss               (medium.com)    \n",
       "187      7          Crusso3     discuss             (circonus.com)    \n",
       "188      8          jukatan   1 comment               (data36.com)    \n",
       "189      9      randyzwitch     discuss                 (mapd.com)    \n",
       "190     10       scotthajek   1 comment           (thenewstack.io)    \n",
       "191     11        analytics     discuss              (statsbot.co)    \n",
       "192     12           kromme     discuss       (theanalyticslab.nl)    \n",
       "193     13         vimarshk     discuss               (medium.com)    \n",
       "194     14         vimarshk     discuss               (medium.com)    \n",
       "195     15           kmax12     discuss   (towardsdatascience.com)    \n",
       "196     16            ddi_1     discuss          (interviewqs.com)    \n",
       "197     17          jukatan     discuss               (data36.com)    \n",
       "198     18          outlace     discuss              (outlace.com)    \n",
       "199     19         italosay     discuss               (github.com)    \n",
       "200     20      andrewxhill     discuss               (medium.com)    \n",
       "201     21      flyelephant     discuss          (flyelephant.net)    \n",
       "202     22         nanonets     discuss               (medium.com)    \n",
       "203     23   SharpSightLabs     discuss       (sharpsightlabs.com)    \n",
       "204     24      tbugaevskii     discuss               (medium.com)    \n",
       "205     25         asdasdas  2 comments               (medium.com)    \n",
       "206     26      tbugaevskii     discuss               (medium.com)    \n",
       "207     27      tbugaevskii     discuss               (medium.com)    \n",
       "208     28        e_ameisen     discuss   (insightdatascience.com)    \n",
       "209     29       adam_alook     discuss       (alookanalytics.com)    \n",
       "\n",
       "                                                 links     points  \\\n",
       "0    https://medium.com/@datalab/aspiring-data-scie...   5 points   \n",
       "1    https://data36.com/sql-for-data-analysis-works...   4 points   \n",
       "2    https://dwhsys.com/2017/03/25/apache-zeppelin-...  23 points   \n",
       "3    https://www.udemy.com/manipulate-excel-file-fr...  26 points   \n",
       "4    https://github.com/axibase/atsd-use-cases/blob...   2 points   \n",
       "5    https://knowtechie.com/mobile-app-business-howto/   2 points   \n",
       "6    https://datahub.packtpub.com/deep-learning/15-...   7 points   \n",
       "7    https://www.mapd.com/blog/exploring-google-ana...   5 points   \n",
       "8    https://drachenbach.github.io/blog/2018/03/12/...   6 points   \n",
       "9    https://medium.com/indeed-data-science/theres-...   4 points   \n",
       "10   https://www.aiworkbox.com/lessons/check-for-el...   2 points   \n",
       "11   http://www.summarizebot.com/summarization_busi...   6 points   \n",
       "12                    https://arxiv.org/abs/1803.10768   2 points   \n",
       "13            https://www.drivendata.org/competitions/   8 points   \n",
       "14           https://kantord.github.io/just-dashboard/   5 points   \n",
       "15   https://blog.insightdatascience.com/hero2vec-d...   8 points   \n",
       "16                https://data36.com/create-table-sql/   2 points   \n",
       "17   https://data36.com/data-science-career-questio...   6 points   \n",
       "18   https://hackernoon.com/aspiring-data-scientist...  14 points   \n",
       "19   https://www.deeplearningtrack.com/single-post/...  16 points   \n",
       "20                     https://www.visualization.bike/  48 points   \n",
       "21    https://statsbot.co/blog/select-random-rows-sql/   2 points   \n",
       "22   https://www.zerotosingularity.com/posts/course...   7 points   \n",
       "23   https://github.com/tmadl/sklearn-interpretable...   7 points   \n",
       "24       https://schedule.sxsw.com/2018/events/PP73084   3 points   \n",
       "25   https://hackernoon.com/how-we-grew-from-0-to-4...   3 points   \n",
       "26                       https://statsbot.co/ltv-ebook   9 points   \n",
       "27   http://randyzwitch.com/mapd-pjm-electricity-data/   7 points   \n",
       "28      https://blog.datalore.io/february-data-digest/   4 points   \n",
       "29   https://medium.com/polyaxon/jupyter-notebooks-...   4 points   \n",
       "..                                                 ...        ...   \n",
       "180  https://blog.insightdatascience.com/how-to-use...   3 points   \n",
       "181  http://tech.marksblogg.com/billion-nyc-taxi-ri...   3 points   \n",
       "182  https://www.interviewqs.com/blog/py_stock_corr...   2 points   \n",
       "183                https://github.com/Italosayan/P-P-P   4 points   \n",
       "184  https://www.mapd.com/blog/scaling-pandas-to-th...   3 points   \n",
       "185  https://medium.com/@plotlygraphs/introducing-p...   3 points   \n",
       "186  https://medium.com/textileio/tutorial-setting-...   3 points   \n",
       "187  https://www.circonus.com/2018/06/prometheus-ad...   2 points   \n",
       "188  https://data36.com/python-libraries-packages-d...   5 points   \n",
       "189     https://www.mapd.com/blog/announcing-mapd-4.0/   4 points   \n",
       "190  https://thenewstack.io/the-good-bad-and-ugly-a...   3 points   \n",
       "191                 https://statsbot.co/blog/data-team   6 points   \n",
       "192  http://www.theanalyticslab.nl/2018/06/24/teleg...   3 points   \n",
       "193  https://medium.com/acing-ai/artificial-intelli...   2 points   \n",
       "194  https://medium.com/acing-ai/walmart-data-scien...   6 points   \n",
       "195  https://towardsdatascience.com/automated-featu...   5 points   \n",
       "196                       https://www.interviewqs.com/   9 points   \n",
       "197  https://courses.data36.com/p/the-junior-data-s...   2 points   \n",
       "198                http://outlace.com/TensorNets1.html   4 points   \n",
       "199      https://github.com/nateraw/Lda2vec-Tensorflow   5 points   \n",
       "200  https://medium.com/textileio/decentralized-cod...   3 points   \n",
       "201  http://datasciencedigest.flyelephant.net/issue...   9 points   \n",
       "202  https://medium.com/nanonets/how-we-flew-a-dron...  12 points   \n",
       "203  http://www.sharpsightlabs.com/blog/key-for-mas...   3 points   \n",
       "204  https://www.medium.com/data-science-school/pra...  26 points   \n",
       "205  https://medium.com/nanonets/topic-modeling-wit...  19 points   \n",
       "206  https://www.medium.com/activewizards-machine-l...  30 points   \n",
       "207  https://www.medium.com/activewizards-machine-l...  21 points   \n",
       "208  https://blog.insightdatascience.com/reinforcem...   8 points   \n",
       "209  https://blog.alookanalytics.com/2018/06/11/geo...   5 points   \n",
       "\n",
       "                                                titles  \n",
       "0    Aspiring Data Scientists! Here are some great ...  \n",
       "1    SQL For Data Analysis Workshop (for Beginners)...  \n",
       "2    Apache Zeppelin vs. Jupyter Notebook: comparis...  \n",
       "3    Automate Excel, Word, PDF, HTML Web Scraping f...  \n",
       "4    Aging America: Modeling Birth Trends in the Un...  \n",
       "5    A 3-step guide to starting your mobile app bus...  \n",
       "6    15 Useful Python Libraries to make your Data S...  \n",
       "7            Exploring Google Analytics data with MapD  \n",
       "8                      The Intuition behind Embeddings  \n",
       "9    Data Science job searching behavior - There's ...  \n",
       "10   Check For Element Wise Equality Between Two Py...  \n",
       "11    Natural language processing and web scraping API  \n",
       "12      The Unreasonable Effectivness of Deep Learning  \n",
       "13   3 new comps for forecasting, anomalies and opt...  \n",
       "14   Just-dashboard: Create shareable interactive d...  \n",
       "15   Predicting e-sports winners with Machine Learning  \n",
       "16                        How to Create a Table in SQL  \n",
       "17                            Is Data Science For You?  \n",
       "18   Aspiring Data Scientists! Start to learn Stati...  \n",
       "19          Setting up spark on google cloud made easy  \n",
       "20   The most complete bike sharing visualization a...  \n",
       "21               Scalable Select of Random Rows in SQL  \n",
       "22   Running your Coursera (and all other) Jupyter ...  \n",
       "23   Simplified tree-based classifier & regressor f...  \n",
       "24   SXSW panel on offline data science, decentrali...  \n",
       "25   Growing from 0 to 4M users on our fashion app ...  \n",
       "26         Estimating customer lifetime value with SQL  \n",
       "27   Getting Started With MapD, Part 2: Electricity...  \n",
       "28   Datalore's February Data Digest - on deep lear...  \n",
       "29       Jupyter notebooks and tensorboard on Polyaxon  \n",
       "..                                                 ...  \n",
       "180  How to use Machine Learning and Quilt to Ident...  \n",
       "181  1.1 Billion Taxi Rides with SQLite, Parquet & ...  \n",
       "182  Correlating stock returns using Python, visual...  \n",
       "183                     Spatiotemporal modeling with R  \n",
       "184  Scaling Pandas to the Billions with Ibis and MapD  \n",
       "185                        Introducing plotly.py 3.0.0  \n",
       "186         Tutorial: Setting up an IPFS peer, part IV  \n",
       "187          Introducing the IRONdb Prometheus Adapter  \n",
       "188  Python libraries and packages for Data Scienti...  \n",
       "189  Announcing MapD 4.0: Geospatial, Role-Based Pe...  \n",
       "190  The Good, Bad and Ugly: Apache Spark for Data ...  \n",
       "191             How to balance the load on a data team  \n",
       "192  Let R/Python send messages when the algorithms...  \n",
       "193  Artificial Intelligence is the Bicycle for our...  \n",
       "194           Walmart Data Science Interview Questions  \n",
       "195            Automated Feature Engineering in Python  \n",
       "196  Practice DS interview questions through email ...  \n",
       "197  The Junior Data Scientist's First Month (Class...  \n",
       "198    Tensor Networks and the Nature of Non-Linearity  \n",
       "199             Simple topic modeling using tensorflow  \n",
       "200  Decentralized code distribution for the future...  \n",
       "201                     DataScience Digest - Issue #13  \n",
       "202  How to easily automate Drone-based monitoring ...  \n",
       "203                   A key for mastering data science  \n",
       "204       Practical Apache Spark in 10 minutes. Part 2  \n",
       "205  How to easily do Topic Modeling with LSA, PSLA...  \n",
       "206            Top 7 Data Science Use Cases in Finance  \n",
       "207  Top 20 R Libraries for Data Science in 2018 [I...  \n",
       "208                Reinforcement Learning from scratch  \n",
       "209  Geolocated nearest neighbors in product campai...  \n",
       "\n",
       "[210 rows x 7 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, numpy as np\n",
    "\n",
    "def parse_url(url=\"http://www.datatau.com\", data=False):\n",
    "    \n",
    "    response  =  requests.get(url)\n",
    "    links     =  Selector(text=response.text).xpath(\"//td[@class='title']/a/@href\").extract()\n",
    "    titles    =  Selector(text=response.text).xpath(\"//td[@class='title']/a/text()\").extract()\n",
    "    points    =  Selector(text=response.text).xpath(\"//td[@class='subtext']/span/text()\").extract()\n",
    "    domains   =  Selector(text=response.text).xpath(\"//td[@class='title']/span/text()\").extract()\n",
    "    authors   =  Selector(text=response.text).xpath(\"//td[@class='subtext']/a[contains(@href, 'user')]/text()\").extract()\n",
    "    comments  =  Selector(text=response.text).xpath(\"//td[@class='subtext']/a[contains(@href, 'item')]/text()\").extract()\n",
    "\n",
    "    expected_length = 30\n",
    "    \n",
    "    # Adding [np.nan]*(expected_length - len(points)) to the end of the lists will fill in missing\n",
    "    # values at the end of results that sometimes don't exist naturally.\n",
    "    scraped = dict(\n",
    "        titles   =  titles[:30], \n",
    "        links    =  links[:30], # :30 Because of that \"more\" link.\n",
    "        points   =  points + [np.nan]*(expected_length - len(points)),\n",
    "        domains  =  domains + [np.nan]*(expected_length - len(domains)),\n",
    "        authors  =  authors + [np.nan]*(expected_length - len(authors)),\n",
    "        comments =  comments + [np.nan]*(expected_length - len(comments))\n",
    "    )\n",
    "    \n",
    "    df = pd.DataFrame(scraped)\n",
    "    \n",
    "    if type(data) != bool:\n",
    "        data = df.append(data)\n",
    "    else:\n",
    "        data = df\n",
    "        \n",
    "    # If there's data, append them. If not, it's the first iteration, so there's no need.\n",
    "    # Find \"more\" link:\n",
    "    more_anchor  =  Selector(text=response.text).xpath(\"//a[text() = 'More']/@href\").extract()\n",
    "    \n",
    "    if len(more_anchor) > 0:\n",
    "        more_url  =  \"http://www.datatau.com%s\" % more_anchor[0]\n",
    "        print \"Fetching %s...\" % more_url\n",
    "        return parse_url(more_url, data=data)\n",
    "    else:\n",
    "        return data.reset_index()\n",
    "       \n",
    "        \n",
    "df = parse_url(\"http://www.datatau.com\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
